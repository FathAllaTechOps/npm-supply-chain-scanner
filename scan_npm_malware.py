#!/usr/bin/env python3
"""
Scan all repositories in a GitHub org for specific malicious npm package versions.

- Lists all repos (handles pagination).
- For each repo, fetches a recursive tree of the default branch to find:
    package-lock.json, yarn.lock, pnpm-lock.yaml, package.json
- Parses/greps files for known-bad package@version combinations.
- Prints a findings table and writes JSON/CSV artifacts.
- Exits with code 1 if anything bad was found, else 0.

Auth: set GITHUB_TOKEN env var (classic or fine-grained with Contents:read).
"""

import csv
import json
import os
import re
import sys
import time
import argparse
import urllib.parse
import urllib.request
from typing import Dict, List, Tuple, Iterable

GITHUB_API = os.environ.get("GITHUB_API", "https://api.github.com")
TOKEN = os.environ.get("GITHUB_TOKEN")

# ----- Known-bad versions from the Sep 8, 2025 incident -----
BAD = {
    "backslash": {"0.2.1"},
    "chalk-template": {"1.1.1"},
    "supports-hyperlinks": {"4.1.1"},
    "has-ansi": {"6.0.1"},
    "simple-swizzle": {"0.2.3"},
    "color-string": {"2.1.1"},
    "error-ex": {"1.3.3"},
    "color-name": {"2.0.1"},
    "is-arrayish": {"0.3.3"},
    "slice-ansi": {"7.1.1"},
    "color-convert": {"3.1.1"},
    "wrap-ansi": {"9.0.1"},
    "ansi-regex": {"6.2.1"},
    "supports-color": {"10.2.1"},
    "strip-ansi": {"7.1.1"},
    "chalk": {"5.6.1"},
    "debug": {"4.4.2"},
    "ansi-styles": {"6.2.2"},
}

# Text-based lockfile regexes
YARN_LOCK_ENTRY = re.compile(
    r'^(?P<key>.+?):\n\s+version\s+"(?P<version>[^"]+)"',
    re.MULTILINE
)
# pnpm-lock.yaml "packages:" entries like '/chalk@5.6.1:'
PNPM_LOCK_ENTRY = re.compile(
    r'^\s*/(?P<name>@?[\w\-/]+)@(?P<version>[0-9]+\.[0-9]+\.[0-9]+):\s*$',
    re.MULTILINE
)

TARGETS = (
    "package-lock.json",
    "yarn.lock",
    "pnpm-lock.yaml",
    "package.json",
)

def gh_request(path: str, params: Dict[str, str] = None) -> Tuple[dict, dict]:
    if not TOKEN:
        print("ERROR: GITHUB_TOKEN is not set.", file=sys.stderr)
        sys.exit(2)
    url = f"{GITHUB_API}{path}"
    if params:
        url += "?" + urllib.parse.urlencode(params)
    req = urllib.request.Request(url)
    req.add_header("Authorization", f"Bearer {TOKEN}")
    req.add_header("Accept", "application/vnd.github+json")
    with urllib.request.urlopen(req) as resp:
        data = json.load(resp)
        headers = {k.lower(): v for k, v in resp.getheaders()}
        return data, headers

def gh_request_raw(path: str, params: Dict[str, str] = None) -> Tuple[bytes, dict]:
    if not TOKEN:
        print("ERROR: GITHUB_TOKEN is not set.", file=sys.stderr)
        sys.exit(2)
    url = f"{GITHUB_API}{path}"
    if params:
        url += "?" + urllib.parse.urlencode(params)
    req = urllib.request.Request(url)
    req.add_header("Authorization", f"Bearer {TOKEN}")
    req.add_header("Accept", "application/vnd.github.raw")
    with urllib.request.urlopen(req) as resp:
        content = resp.read()
        headers = {k.lower(): v for k, v in resp.getheaders()}
        return content, headers

def handle_rate_limit(headers: dict):
    remaining = headers.get("x-ratelimit-remaining")
    reset = headers.get("x-ratelimit-reset")
    if remaining is not None and reset is not None:
        try:
            rem = int(remaining)
            rst = int(reset)
            if rem <= 2:
                sleep_for = max(rst - int(time.time()), 1)
                print(f"[rate] Near limit; sleeping {sleep_for}sâ€¦", file=sys.stderr)
                time.sleep(sleep_for)
        except ValueError:
            pass

def list_org_repos(org: str, include_archived: bool, max_repos: int) -> List[dict]:
    repos = []
    page = 1
    per_page = 100
    while True:
        data, headers = gh_request(f"/orgs/{org}/repos",
                                   {"per_page": per_page, "page": page, "type": "all", "sort": "full_name"})
        handle_rate_limit(headers)
        if not data:
            break
        for r in data:
            if not include_archived and r.get("archived"):
                continue
            repos.append(r)
            if len(repos) >= max_repos:
                return repos
        page += 1
    return repos

def get_default_branch(owner: str, repo: str) -> str:
    data, headers = gh_request(f"/repos/{owner}/{repo}")
    handle_rate_limit(headers)
    return data.get("default_branch", "main")

def get_tree(owner: str, repo: str, ref: str) -> List[dict]:
    data, headers = gh_request(f"/repos/{owner}/{repo}/git/trees/{ref}", {"recursive": "1"})
    handle_rate_limit(headers)
    return data.get("tree", [])

def get_file(owner: str, repo: str, path: str, ref: str) -> bytes:
    content, headers = gh_request_raw(f"/repos/{owner}/{repo}/contents/{path}", {"ref": ref})
    handle_rate_limit(headers)
    return content

def parse_package_lock(blob: str) -> Iterable[Tuple[str, str]]:
    try:
        data = json.loads(blob)
    except json.JSONDecodeError:
        return []
    found = []
    if "packages" in data and isinstance(data["packages"], dict):
        for _, meta in data["packages"].items():
            name = meta.get("name")
            ver = meta.get("version")
            if name and ver:
                found.append((name, str(ver)))
    elif "dependencies" in data and isinstance(data["dependencies"], dict):
        def walk_deps(d: dict):
            for name, meta in d.items():
                ver = meta.get("version")
                if ver:
                    yield (name, str(ver))
                if "dependencies" in meta:
                    yield from walk_deps(meta["dependencies"])
        found.extend(list(walk_deps(data["dependencies"])))
    return found

def parse_yarn_lock(blob: str) -> Iterable[Tuple[str, str]]:
    for m in YARN_LOCK_ENTRY.finditer(blob):
        key = m.group("key").strip()
        version = m.group("version").strip()
        # choose first key if multiple separated by ", "
        name_part = key.split(",")[0].strip()
        if name_part.startswith("@"):
            at2 = name_part.find("@", 1)
            name = name_part[:at2] if at2 != -1 else name_part
        else:
            parts = name_part.split("@", 1)
            name = parts[0] if parts else name_part
        if name:
            yield (name, version)

def parse_pnpm_lock(blob: str) -> Iterable[Tuple[str, str]]:
    for m in PNPM_LOCK_ENTRY.finditer(blob):
        yield (m.group("name"), m.group("version"))

def parse_package_json(blob: str) -> Iterable[Tuple[str, str]]:
    try:
        data = json.loads(blob)
    except json.JSONDecodeError:
        return []
    found = []
    for field in ("dependencies", "devDependencies", "optionalDependencies", "peerDependencies"):
        deps = data.get(field, {})
        if isinstance(deps, dict):
            for name, spec in deps.items():
                spec_str = str(spec).strip()
                # Only flag exact pins; ranges resolved by lockfiles
                if spec_str and not spec_str.startswith(("^", "~", ">", "<", "=")):
                    found.append((name, spec_str))
    return found

def check_pairs(pairs: Iterable[Tuple[str, str]]) -> List[Tuple[str, str]]:
    bad = []
    for name, ver in pairs:
        name_l = name.strip()
        ver_l = ver.strip()
        if name_l in BAD and ver_l in BAD[name_l]:
            bad.append((name_l, ver_l))
    return bad

def write_json(findings: List[dict], path: str):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(findings, f, indent=2, ensure_ascii=False)

def write_csv(findings: List[dict], path: str):
    # rows: repo, branch, file, package, version
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["repo", "branch", "file", "package", "version"])
        for fnd in findings:
            repo = fnd["repo"]
            branch = fnd["branch"]
            for h in fnd["hits"]:
                w.writerow([repo, branch, h["file"], h["package"], h["version"]])

def main():
    parser = argparse.ArgumentParser(description="Scan GitHub org repos for malicious npm versions.")
    parser.add_argument("--org", required=True, help="GitHub organization login")
    parser.add_argument("--include-archived", action="store_true", help="Scan archived repos too")
    parser.add_argument("--max-repos", type=int, default=10000, help="Ceiling for repos to scan")
    parser.add_argument("--verbose", action="store_true")
    parser.add_argument("--json-out", default="findings.json", help="Path to JSON output")
    parser.add_argument("--csv-out", default="findings.csv", help="Path to CSV output")
    args = parser.parse_args()

    repos = list_org_repos(args.org, args.include_archived, args.max_repos)
    if not repos:
        print("No repos found or access denied.")
        sys.exit(0)

    findings = []  # list of {repo, branch, hits:[{file, package, version}]}

    for r in repos:
        owner = r["owner"]["login"]
        name = r["name"]
        default_branch = r.get("default_branch") or get_default_branch(owner, name)
        if args.verbose:
            print(f"[scan] {owner}/{name}@{default_branch}")

        # Get full tree recursively
        try:
            tree = get_tree(owner, name, default_branch)
        except Exception as e:
            print(f"[warn] failed to get tree for {owner}/{name}: {e}", file=sys.stderr)
            continue

        # Candidate files anywhere in the repo
        paths = [t["path"] for t in tree if t.get("type") == "blob" and any(t["path"].endswith(x) for x in TARGETS)]
        if not paths:
            continue

        repo_hits = []
        for path in paths:
            try:
                raw = get_file(owner, name, path, default_branch)
            except Exception as e:
                if args.verbose:
                    print(f"[warn] failed to get {owner}/{name}:{path}: {e}", file=sys.stderr)
                continue

            bad_pairs = []
            try:
                if path.endswith("package-lock.json"):
                    pairs = parse_package_lock(raw.decode("utf-8", errors="ignore"))
                    bad_pairs = check_pairs(pairs)

                elif path.endswith("yarn.lock"):
                    text = raw.decode("utf-8", errors="ignore")
                    pairs = list(parse_yarn_lock(text))
                    bad_pairs = check_pairs(pairs)

                elif path.endswith("pnpm-lock.yaml"):
                    text = raw.decode("utf-8", errors="ignore")
                    pairs = list(parse_pnpm_lock(text))
                    bad_pairs = check_pairs(pairs)

                elif path.endswith("package.json"):
                    pairs = parse_package_json(raw.decode("utf-8", errors="ignore"))
                    bad_pairs = check_pairs(pairs)
            except Exception as e:
                if args.verbose:
                    print(f"[warn] parse error for {owner}/{name}:{path}: {e}", file=sys.stderr)

            if bad_pairs:
                for (pkg, ver) in bad_pairs:
                    repo_hits.append({
                        "file": path,           # full nested path preserved
                        "package": pkg,
                        "version": ver
                    })

        if repo_hits:
            findings.append({
                "repo": f"{owner}/{name}",
                "branch": default_branch,
                "hits": repo_hits
            })

    # Output
    if findings:
        print("\n=== DETECTED MALICIOUS VERSIONS ===")
        total_hits = 0
        for fnd in findings:
            print(f"- {fnd['repo']}@{fnd['branch']}")
            for h in fnd["hits"]:
                total_hits += 1
                print(f"    {h['file']}: {h['package']}@{h['version']}")
        print(f"\nTotal affected repos: {len(findings)} | Total hits: {total_hits}")

        # Write artifacts
        write_json(findings, args.json_out)
        write_csv(findings, args.csv_out)
        print(f"\nArtifacts written: {args.json_out}, {args.csv_out}")
        sys.exit(1)
    else:
        print("No malicious versions detected in scanned repositories.")
        # still write empty artifacts for bookkeeping
        write_json([], args.json_out)
        write_csv([], args.csv_out)
        sys.exit(0)

if __name__ == "__main__":
    main()
