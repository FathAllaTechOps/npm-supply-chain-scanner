#!/usr/bin/env python3
"""
Scan all repositories in a GitHub org for specific malicious npm package versions.

- Lists all repos (handles pagination).
- For each repo, fetches a recursive tree of the default branch to find:
    package-lock.json, yarn.lock, pnpm-lock.yaml, package.json
- Parses/greps files for known-bad package@version combinations.
- Prints a findings table + JSON summary.
- Exits with code 1 if anything bad was found, else 0.

Auth: set GITHUB_TOKEN env var (classic or fine-grained with Contents:read).
"""

import base64
import json
import os
import re
import sys
import time
import argparse
import urllib.parse
import urllib.request
from typing import Dict, List, Tuple, Iterable

GITHUB_API = os.environ.get("GITHUB_API", "https://api.github.com")
TOKEN = os.environ.get("GITHUB_TOKEN")

# ----- Known-bad versions from the Sep 8, 2025 incident -----
# Keep this list as exact pins; lockfiles should show exact resolved versions.
BAD = {
    "backslash": {"0.2.1"},
    "chalk-template": {"1.1.1"},
    "supports-hyperlinks": {"4.1.1"},
    "has-ansi": {"6.0.1"},
    "simple-swizzle": {"0.2.3"},
    "color-string": {"2.1.1"},
    "error-ex": {"1.3.3"},
    "color-name": {"2.0.1"},
    "is-arrayish": {"0.3.3"},
    "slice-ansi": {"7.1.1"},
    "color-convert": {"3.1.1"},
    "wrap-ansi": {"9.0.1"},
    "ansi-regex": {"6.2.1"},
    "supports-color": {"10.2.1"},
    "strip-ansi": {"7.1.1"},
    "chalk": {"5.6.1"},
    "debug": {"4.4.2"},
    "ansi-styles": {"6.2.2"},
}

# Regexes for text-based lockfiles
YARN_LOCK_ENTRY = re.compile(
    r'^(?P<key>.+?):\n\s+version\s+"(?P<version>[^"]+)"',
    re.MULTILINE
)
# pnpm-lock.yaml "packages:" entries look like:
# packages:
#   /chalk@5.6.1:
#     resolution: {integrity: sha512-...}
PNPM_LOCK_ENTRY = re.compile(
    r'^\s*/(?P<name>@?[\w\-/]+)@(?P<version>[0-9]+\.[0-9]+\.[0-9]+):\s*$',
    re.MULTILINE
)

def gh_request(path: str, params: Dict[str, str] = None) -> Tuple[dict, dict]:
    if not TOKEN:
        print("ERROR: GITHUB_TOKEN is not set.", file=sys.stderr)
        sys.exit(2)

    url = f"{GITHUB_API}{path}"
    if params:
        url += "?" + urllib.parse.urlencode(params)
    req = urllib.request.Request(url)
    req.add_header("Authorization", f"Bearer {TOKEN}")
    req.add_header("Accept", "application/vnd.github+json")
    with urllib.request.urlopen(req) as resp:
        data = json.load(resp)
        headers = {k.lower(): v for k, v in resp.getheaders()}
        return data, headers

def gh_request_raw(path: str, params: Dict[str, str] = None) -> Tuple[bytes, dict]:
    if not TOKEN:
        print("ERROR: GITHUB_TOKEN is not set.", file=sys.stderr)
        sys.exit(2)

    url = f"{GITHUB_API}{path}"
    if params:
        url += "?" + urllib.parse.urlencode(params)
    req = urllib.request.Request(url)
    req.add_header("Authorization", f"Bearer {TOKEN}")
    req.add_header("Accept", "application/vnd.github.raw")
    with urllib.request.urlopen(req) as resp:
        content = resp.read()
        headers = {k.lower(): v for k, v in resp.getheaders()}
        return content, headers

def handle_rate_limit(headers: dict):
    # Basic backoff if we’re close to rate limit
    remaining = headers.get("x-ratelimit-remaining")
    reset = headers.get("x-ratelimit-reset")
    if remaining is not None and reset is not None:
        try:
            rem = int(remaining)
            rst = int(reset)
            if rem <= 2:
                sleep_for = max(rst - int(time.time()), 1)
                print(f"[rate] Near limit; sleeping {sleep_for}s…", file=sys.stderr)
                time.sleep(sleep_for)
        except ValueError:
            pass

def list_org_repos(org: str, include_archived: bool, max_repos: int) -> List[dict]:
    repos = []
    page = 1
    per_page = 100
    while True:
        data, headers = gh_request(f"/orgs/{org}/repos",
                                   {"per_page": per_page, "page": page, "type": "all"})
        handle_rate_limit(headers)
        if not data:
            break
        for r in data:
            if not include_archived and r.get("archived"):
                continue
            repos.append(r)
            if len(repos) >= max_repos:
                return repos
        page += 1
    return repos

def get_default_branch(owner: str, repo: str) -> str:
    data, headers = gh_request(f"/repos/{owner}/{repo}")
    handle_rate_limit(headers)
    return data.get("default_branch", "main")

def get_tree(owner: str, repo: str, ref: str) -> List[dict]:
    data, headers = gh_request(f"/repos/{owner}/{repo}/git/trees/{ref}", {"recursive": "1"})
    handle_rate_limit(headers)
    return data.get("tree", [])

def get_file(owner: str, repo: str, path: str, ref: str) -> bytes:
    # Use raw contents API
    content, headers = gh_request_raw(f"/repos/{owner}/{repo}/contents/{path}", {"ref": ref})
    handle_rate_limit(headers)
    return content

def parse_package_lock(blob: str) -> Iterable[Tuple[str, str]]:
    """
    package-lock.json (npm v2+): prefer 'packages' map (keys are paths).
    fallback to 'dependencies' if needed.
    """
    try:
        data = json.loads(blob)
    except json.JSONDecodeError:
        return []
    found = []
    if "packages" in data and isinstance(data["packages"], dict):
        for pkg_path, meta in data["packages"].items():
            # meta may include "name" and "version"
            name = meta.get("name")
            ver = meta.get("version")
            if name and ver:
                found.append((name, str(ver)))
    elif "dependencies" in data and isinstance(data["dependencies"], dict):
        # older format
        def walk_deps(d: dict):
            for name, meta in d.items():
                ver = meta.get("version")
                if ver:
                    yield (name, str(ver))
                if "dependencies" in meta:
                    yield from walk_deps(meta["dependencies"])
        found.extend(list(walk_deps(data["dependencies"])))
    return found

def parse_yarn_lock(blob: str) -> Iterable[Tuple[str, str]]:
    """
    yarn.lock generic: extract 'version "<x.y.z>"' entries.
    Note: keys may have multiple semver ranges; we only need name + version.
    """
    for m in YARN_LOCK_ENTRY.finditer(blob):
        key = m.group("key").strip()
        version = m.group("version").strip()
        # key can be "chalk@^5.0.0", "chalk@npm:^5", or multiple entries separated by ", "
        # Extract package name as text before first '@' when possible, but scoped names complicate that.
        # Heuristic: last occurrence of "@" separates name/range for scoped or unscoped.
        name_part = key.split(",")[0].strip()  # first entry if multiple
        # For scoped packages, name begins with "@scope/name"
        if name_part.startswith("@"):
            # find second '@' which separates range in yarn keys
            # examples: "@scope/name@^1.2.3"
            at2 = name_part.find("@", 1)
            if at2 != -1:
                name = name_part[:at2]
            else:
                # fallback (rare)
                name = name_part
        else:
            # unscoped: split at first '@'
            parts = name_part.split("@", 1)
            name = parts[0] if parts else name_part
        if name:
            yield (name, version)

def parse_pnpm_lock(blob: str) -> Iterable[Tuple[str, str]]:
    """
    pnpm-lock.yaml: look for lines like '/chalk@5.6.1:'
    """
    for m in PNPM_LOCK_ENTRY.finditer(blob):
        yield (m.group("name"), m.group("version"))

def parse_package_json(blob: str) -> Iterable[Tuple[str, str]]:
    """
    package.json: check deps/devDeps for exact pins.
    (Caret/tilde ranges won’t match here; lockfiles are more authoritative.)
    """
    try:
        data = json.loads(blob)
    except json.JSONDecodeError:
        return []
    found = []
    for field in ("dependencies", "devDependencies", "optionalDependencies", "peerDependencies"):
        deps = data.get(field, {})
        if isinstance(deps, dict):
            for name, spec in deps.items():
                # Only flag exact versions that match known-bad
                # If spec includes ^ or ~, rely on lockfile detection instead.
                spec_str = str(spec).strip()
                if spec_str and not spec_str.startswith(("^", "~", ">", "<", "=")):
                    found.append((name, spec_str))
    return found

def check_pairs(pairs: Iterable[Tuple[str, str]]) -> List[Tuple[str, str]]:
    bad = []
    for name, ver in pairs:
        name_l = name.strip()
        ver_l = ver.strip()
        if name_l in BAD and ver_l in BAD[name_l]:
            bad.append((name_l, ver_l))
    return bad

def main():
    parser = argparse.ArgumentParser(description="Scan GitHub org repos for malicious npm versions.")
    parser.add_argument("--org", required=True, help="GitHub organization login")
    parser.add_argument("--include-archived", action="store_true", help="Scan archived repos too")
    parser.add_argument("--max-repos", type=int, default=10000, help="Ceiling for repos to scan")
    parser.add_argument("--verbose", action="store_true")
    args = parser.parse_args()

    repos = list_org_repos(args.org, args.include_archived, args.max_repos)
    if not repos:
        print("No repos found or access denied.")
        sys.exit(0)

    findings = []  # list of dicts
    targets = (
        "package-lock.json",
        "yarn.lock",
        "pnpm-lock.yaml",
        "package.json",
    )

    for r in repos:
        owner = r["owner"]["login"]
        name = r["name"]
        default_branch = r.get("default_branch") or get_default_branch(owner, name)
        if args.verbose:
            print(f"[scan] {owner}/{name}@{default_branch}")

        # Fetch tree
        try:
            tree = get_tree(owner, name, default_branch)
        except Exception as e:
            print(f"[warn] failed to get tree for {owner}/{name}: {e}", file=sys.stderr)
            continue

        # Find candidate files
        paths = [t["path"] for t in tree if t.get("type") == "blob" and any(t["path"].endswith(x) for x in targets)]
        if not paths:
            continue

        repo_hits = []
        for path in paths:
            try:
                raw = get_file(owner, name, path, default_branch)
            except Exception as e:
                if args.verbose:
                    print(f"[warn] failed to get {owner}/{name}:{path}: {e}", file=sys.stderr)
                continue

            text = None
            bad_pairs = []

            if path.endswith("package-lock.json"):
                try:
                    pairs = parse_package_lock(raw.decode("utf-8", errors="ignore"))
                    bad_pairs = check_pairs(pairs)
                except Exception as e:
                    if args.verbose:
                        print(f"[warn] parse package-lock.json error: {e}", file=sys.stderr)

            elif path.endswith("yarn.lock"):
                text = raw.decode("utf-8", errors="ignore")
                pairs = list(parse_yarn_lock(text))
                bad_pairs = check_pairs(pairs)

            elif path.endswith("pnpm-lock.yaml"):
                text = raw.decode("utf-8", errors="ignore")
                pairs = list(parse_pnpm_lock(text))
                bad_pairs = check_pairs(pairs)

            elif path.endswith("package.json"):
                pairs = parse_package_json(raw.decode("utf-8", errors="ignore"))
                bad_pairs = check_pairs(pairs)

            if bad_pairs:
                for (pkg, ver) in bad_pairs:
                    repo_hits.append({
                        "file": path,
                        "package": pkg,
                        "version": ver
                    })

        if repo_hits:
            findings.append({
                "repo": f"{owner}/{name}",
                "branch": default_branch,
                "hits": repo_hits
            })

    # Output
    if findings:
        print("\n=== DETECTED MALICIOUS VERSIONS ===")
        for f in findings:
            print(f"- {f['repo']}@{f['branch']}")
            for h in f["hits"]:
                print(f"    {h['file']}: {h['package']}@{h['version']}")
        print("\nJSON summary:")
        print(json.dumps(findings, indent=2))
        sys.exit(1)
    else:
        print("No malicious versions detected in scanned repositories.")
        sys.exit(0)

if __name__ == "__main__":
    main()
